{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras_colab_Uebung3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RichardKGitHub/ProgrammingGit/blob/master/Keras_colab_Uebung3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "BgSwNUsWSsFm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn import svm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7v3xPPLwbqNW",
        "colab_type": "code",
        "outputId": "a0c112a9-0e3c-44e0-ea10-4f50772185dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "cell_type": "code",
      "source": [
        "dir(svm)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['LinearSVC',\n",
              " 'LinearSVR',\n",
              " 'NuSVC',\n",
              " 'NuSVR',\n",
              " 'OneClassSVM',\n",
              " 'SVC',\n",
              " 'SVR',\n",
              " '__all__',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__path__',\n",
              " '__spec__',\n",
              " 'base',\n",
              " 'bounds',\n",
              " 'classes',\n",
              " 'l1_min_c',\n",
              " 'liblinear',\n",
              " 'libsvm',\n",
              " 'libsvm_sparse']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "Iv7WicKWadfI",
        "colab_type": "code",
        "outputId": "d10b3155-0426-44f2-d684-8f1cb5da3909",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34530
        }
      },
      "cell_type": "code",
      "source": [
        "help(svm.classes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on module sklearn.svm.classes in sklearn.svm:\n",
            "\n",
            "NAME\n",
            "    sklearn.svm.classes\n",
            "\n",
            "CLASSES\n",
            "    sklearn.base.BaseEstimator(builtins.object)\n",
            "        LinearSVC(sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.linear_model.base.SparseCoefMixin)\n",
            "    sklearn.base.RegressorMixin(builtins.object)\n",
            "        LinearSVR(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
            "        NuSVR(sklearn.svm.base.BaseLibSVM, sklearn.base.RegressorMixin)\n",
            "        SVR(sklearn.svm.base.BaseLibSVM, sklearn.base.RegressorMixin)\n",
            "    sklearn.linear_model.base.LinearClassifierMixin(sklearn.base.ClassifierMixin)\n",
            "        LinearSVC(sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.linear_model.base.SparseCoefMixin)\n",
            "    sklearn.linear_model.base.LinearModel(abc.NewBase)\n",
            "        LinearSVR(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
            "    sklearn.linear_model.base.SparseCoefMixin(builtins.object)\n",
            "        LinearSVC(sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.linear_model.base.SparseCoefMixin)\n",
            "    sklearn.svm.base.BaseLibSVM(abc.NewBase)\n",
            "        NuSVR(sklearn.svm.base.BaseLibSVM, sklearn.base.RegressorMixin)\n",
            "        OneClassSVM\n",
            "        SVR(sklearn.svm.base.BaseLibSVM, sklearn.base.RegressorMixin)\n",
            "    sklearn.svm.base.BaseSVC(abc.NewBase)\n",
            "        NuSVC\n",
            "        SVC\n",
            "    \n",
            "    class LinearSVC(sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.linear_model.base.SparseCoefMixin)\n",
            "     |  Linear Support Vector Classification.\n",
            "     |  \n",
            "     |  Similar to SVC with parameter kernel='linear', but implemented in terms of\n",
            "     |  liblinear rather than libsvm, so it has more flexibility in the choice of\n",
            "     |  penalties and loss functions and should scale better to large numbers of\n",
            "     |  samples.\n",
            "     |  \n",
            "     |  This class supports both dense and sparse input and the multiclass support\n",
            "     |  is handled according to a one-vs-the-rest scheme.\n",
            "     |  \n",
            "     |  Read more in the :ref:`User Guide <svm_classification>`.\n",
            "     |  \n",
            "     |  Parameters\n",
            "     |  ----------\n",
            "     |  penalty : string, 'l1' or 'l2' (default='l2')\n",
            "     |      Specifies the norm used in the penalization. The 'l2'\n",
            "     |      penalty is the standard used in SVC. The 'l1' leads to ``coef_``\n",
            "     |      vectors that are sparse.\n",
            "     |  \n",
            "     |  loss : string, 'hinge' or 'squared_hinge' (default='squared_hinge')\n",
            "     |      Specifies the loss function. 'hinge' is the standard SVM loss\n",
            "     |      (used e.g. by the SVC class) while 'squared_hinge' is the\n",
            "     |      square of the hinge loss.\n",
            "     |  \n",
            "     |  dual : bool, (default=True)\n",
            "     |      Select the algorithm to either solve the dual or primal\n",
            "     |      optimization problem. Prefer dual=False when n_samples > n_features.\n",
            "     |  \n",
            "     |  tol : float, optional (default=1e-4)\n",
            "     |      Tolerance for stopping criteria.\n",
            "     |  \n",
            "     |  C : float, optional (default=1.0)\n",
            "     |      Penalty parameter C of the error term.\n",
            "     |  \n",
            "     |  multi_class : string, 'ovr' or 'crammer_singer' (default='ovr')\n",
            "     |      Determines the multi-class strategy if `y` contains more than\n",
            "     |      two classes.\n",
            "     |      ``\"ovr\"`` trains n_classes one-vs-rest classifiers, while\n",
            "     |      ``\"crammer_singer\"`` optimizes a joint objective over all classes.\n",
            "     |      While `crammer_singer` is interesting from a theoretical perspective\n",
            "     |      as it is consistent, it is seldom used in practice as it rarely leads\n",
            "     |      to better accuracy and is more expensive to compute.\n",
            "     |      If ``\"crammer_singer\"`` is chosen, the options loss, penalty and dual\n",
            "     |      will be ignored.\n",
            "     |  \n",
            "     |  fit_intercept : boolean, optional (default=True)\n",
            "     |      Whether to calculate the intercept for this model. If set\n",
            "     |      to false, no intercept will be used in calculations\n",
            "     |      (i.e. data is expected to be already centered).\n",
            "     |  \n",
            "     |  intercept_scaling : float, optional (default=1)\n",
            "     |      When self.fit_intercept is True, instance vector x becomes\n",
            "     |      ``[x, self.intercept_scaling]``,\n",
            "     |      i.e. a \"synthetic\" feature with constant value equals to\n",
            "     |      intercept_scaling is appended to the instance vector.\n",
            "     |      The intercept becomes intercept_scaling * synthetic feature weight\n",
            "     |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
            "     |      as all other features.\n",
            "     |      To lessen the effect of regularization on synthetic feature weight\n",
            "     |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
            "     |  \n",
            "     |  class_weight : {dict, 'balanced'}, optional\n",
            "     |      Set the parameter C of class i to ``class_weight[i]*C`` for\n",
            "     |      SVC. If not given, all classes are supposed to have\n",
            "     |      weight one.\n",
            "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
            "     |      weights inversely proportional to class frequencies in the input data\n",
            "     |      as ``n_samples / (n_classes * np.bincount(y))``\n",
            "     |  \n",
            "     |  verbose : int, (default=0)\n",
            "     |      Enable verbose output. Note that this setting takes advantage of a\n",
            "     |      per-process runtime setting in liblinear that, if enabled, may not work\n",
            "     |      properly in a multithreaded context.\n",
            "     |  \n",
            "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
            "     |      The seed of the pseudo random number generator to use when shuffling\n",
            "     |      the data.  If int, random_state is the seed used by the random number\n",
            "     |      generator; If RandomState instance, random_state is the random number\n",
            "     |      generator; If None, the random number generator is the RandomState\n",
            "     |      instance used by `np.random`.\n",
            "     |  \n",
            "     |  max_iter : int, (default=1000)\n",
            "     |      The maximum number of iterations to be run.\n",
            "     |  \n",
            "     |  Attributes\n",
            "     |  ----------\n",
            "     |  coef_ : array, shape = [n_features] if n_classes == 2 else [n_classes, n_features]\n",
            "     |      Weights assigned to the features (coefficients in the primal\n",
            "     |      problem). This is only available in the case of a linear kernel.\n",
            "     |  \n",
            "     |      ``coef_`` is a readonly property derived from ``raw_coef_`` that\n",
            "     |      follows the internal memory layout of liblinear.\n",
            "     |  \n",
            "     |  intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]\n",
            "     |      Constants in decision function.\n",
            "     |  \n",
            "     |  Examples\n",
            "     |  --------\n",
            "     |  >>> from sklearn.svm import LinearSVC\n",
            "     |  >>> from sklearn.datasets import make_classification\n",
            "     |  >>> X, y = make_classification(n_features=4, random_state=0)\n",
            "     |  >>> clf = LinearSVC(random_state=0)\n",
            "     |  >>> clf.fit(X, y)\n",
            "     |  LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
            "     |       intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
            "     |       multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n",
            "     |       verbose=0)\n",
            "     |  >>> print(clf.coef_)\n",
            "     |  [[ 0.08551385  0.39414796  0.49847831  0.37513797]]\n",
            "     |  >>> print(clf.intercept_)\n",
            "     |  [ 0.28418066]\n",
            "     |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
            "     |  [1]\n",
            "     |  \n",
            "     |  Notes\n",
            "     |  -----\n",
            "     |  The underlying C implementation uses a random number generator to\n",
            "     |  select features when fitting the model. It is thus not uncommon\n",
            "     |  to have slightly different results for the same input data. If\n",
            "     |  that happens, try with a smaller ``tol`` parameter.\n",
            "     |  \n",
            "     |  The underlying implementation, liblinear, uses a sparse internal\n",
            "     |  representation for the data that will incur a memory copy.\n",
            "     |  \n",
            "     |  Predict output may not match that of standalone liblinear in certain\n",
            "     |  cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
            "     |  in the narrative documentation.\n",
            "     |  \n",
            "     |  References\n",
            "     |  ----------\n",
            "     |  `LIBLINEAR: A Library for Large Linear Classification\n",
            "     |  <http://www.csie.ntu.edu.tw/~cjlin/liblinear/>`__\n",
            "     |  \n",
            "     |  See also\n",
            "     |  --------\n",
            "     |  SVC\n",
            "     |      Implementation of Support Vector Machine classifier using libsvm:\n",
            "     |      the kernel can be non-linear but its SMO algorithm does not\n",
            "     |      scale to large number of samples as LinearSVC does.\n",
            "     |  \n",
            "     |      Furthermore SVC multi-class mode is implemented using one\n",
            "     |      vs one scheme while LinearSVC uses one vs the rest. It is\n",
            "     |      possible to implement one vs the rest with SVC by using the\n",
            "     |      :class:`sklearn.multiclass.OneVsRestClassifier` wrapper.\n",
            "     |  \n",
            "     |      Finally SVC can fit dense data without memory copy if the input\n",
            "     |      is C-contiguous. Sparse data will still incur memory copy though.\n",
            "     |  \n",
            "     |  sklearn.linear_model.SGDClassifier\n",
            "     |      SGDClassifier can optimize the same cost function as LinearSVC\n",
            "     |      by adjusting the penalty and loss parameters. In addition it requires\n",
            "     |      less memory, allows incremental (online) learning, and implements\n",
            "     |      various loss functions and regularization regimes.\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      LinearSVC\n",
            "     |      sklearn.base.BaseEstimator\n",
            "     |      sklearn.linear_model.base.LinearClassifierMixin\n",
            "     |      sklearn.base.ClassifierMixin\n",
            "     |      sklearn.linear_model.base.SparseCoefMixin\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __init__(self, penalty='l2', loss='squared_hinge', dual=True, tol=0.0001, C=1.0, multi_class='ovr', fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, random_state=None, max_iter=1000)\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |  \n",
            "     |  fit(self, X, y, sample_weight=None)\n",
            "     |      Fit the model according to the given training data.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
            "     |          Training vector, where n_samples in the number of samples and\n",
            "     |          n_features is the number of features.\n",
            "     |      \n",
            "     |      y : array-like, shape = [n_samples]\n",
            "     |          Target vector relative to X\n",
            "     |      \n",
            "     |      sample_weight : array-like, shape = [n_samples], optional\n",
            "     |          Array of weights that are assigned to individual\n",
            "     |          samples. If not provided,\n",
            "     |          then each sample is given unit weight.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self : object\n",
            "     |          Returns self.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
            "     |  \n",
            "     |  __getstate__(self)\n",
            "     |  \n",
            "     |  __repr__(self)\n",
            "     |      Return repr(self).\n",
            "     |  \n",
            "     |  __setstate__(self, state)\n",
            "     |  \n",
            "     |  get_params(self, deep=True)\n",
            "     |      Get parameters for this estimator.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      deep : boolean, optional\n",
            "     |          If True, will return the parameters for this estimator and\n",
            "     |          contained subobjects that are estimators.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      params : mapping of string to any\n",
            "     |          Parameter names mapped to their values.\n",
            "     |  \n",
            "     |  set_params(self, **params)\n",
            "     |      Set the parameters of this estimator.\n",
            "     |      \n",
            "     |      The method works on simple estimators as well as on nested objects\n",
            "     |      (such as pipelines). The latter have parameters of the form\n",
            "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
            "     |      component of a nested object.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.linear_model.base.LinearClassifierMixin:\n",
            "     |  \n",
            "     |  decision_function(self, X)\n",
            "     |      Predict confidence scores for samples.\n",
            "     |      \n",
            "     |      The confidence score for a sample is the signed distance of that\n",
            "     |      sample to the hyperplane.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
            "     |          Samples.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
            "     |          Confidence scores per (sample, class) combination. In the binary\n",
            "     |          case, confidence score for self.classes_[1] where >0 means this\n",
            "     |          class would be predicted.\n",
            "     |  \n",
            "     |  predict(self, X)\n",
            "     |      Predict class labels for samples in X.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
            "     |          Samples.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      C : array, shape = [n_samples]\n",
            "     |          Predicted class label per sample.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
            "     |  \n",
            "     |  score(self, X, y, sample_weight=None)\n",
            "     |      Returns the mean accuracy on the given test data and labels.\n",
            "     |      \n",
            "     |      In multi-label classification, this is the subset accuracy\n",
            "     |      which is a harsh metric since you require for each sample that\n",
            "     |      each label set be correctly predicted.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X : array-like, shape = (n_samples, n_features)\n",
            "     |          Test samples.\n",
            "     |      \n",
            "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
            "     |          True labels for X.\n",
            "     |      \n",
            "     |      sample_weight : array-like, shape = [n_samples], optional\n",
            "     |          Sample weights.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      score : float\n",
            "     |          Mean accuracy of self.predict(X) wrt. y.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.linear_model.base.SparseCoefMixin:\n",
            "     |  \n",
            "     |  densify(self)\n",
            "     |      Convert coefficient matrix to dense array format.\n",
            "     |      \n",
            "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
            "     |      default format of ``coef_`` and is required for fitting, so calling\n",
            "     |      this method is only required on models that have previously been\n",
            "     |      sparsified; otherwise, it is a no-op.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self : estimator\n",
            "     |  \n",
            "     |  sparsify(self)\n",
            "     |      Convert coefficient matrix to sparse format.\n",
            "     |      \n",
            "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
            "     |      L1-regularized models can be much more memory- and storage-efficient\n",
            "     |      than the usual numpy.ndarray representation.\n",
            "     |      \n",
            "     |      The ``intercept_`` member is not converted.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
            "     |      this may actually *increase* memory usage, so use this method with\n",
            "     |      care. A rule of thumb is that the number of zero elements, which can\n",
            "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
            "     |      to provide significant benefits.\n",
            "     |      \n",
            "     |      After calling this method, further fitting with the partial_fit\n",
            "     |      method (if any) will not work until you call densify.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self : estimator\n",
            "    \n",
            "    class LinearSVR(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
            "     |  Linear Support Vector Regression.\n",
            "     |  \n",
            "     |  Similar to SVR with parameter kernel='linear', but implemented in terms of\n",
            "     |  liblinear rather than libsvm, so it has more flexibility in the choice of\n",
            "     |  penalties and loss functions and should scale better to large numbers of\n",
            "     |  samples.\n",
            "     |  \n",
            "     |  This class supports both dense and sparse input.\n",
            "     |  \n",
            "     |  Read more in the :ref:`User Guide <svm_regression>`.\n",
            "     |  \n",
            "     |  Parameters\n",
            "     |  ----------\n",
            "     |  C : float, optional (default=1.0)\n",
            "     |      Penalty parameter C of the error term. The penalty is a squared\n",
            "     |      l2 penalty. The bigger this parameter, the less regularization is used.\n",
            "     |  \n",
            "     |  loss : string, 'epsilon_insensitive' or 'squared_epsilon_insensitive' (default='epsilon_insensitive')\n",
            "     |      Specifies the loss function. 'l1' is the epsilon-insensitive loss\n",
            "     |      (standard SVR) while 'l2' is the squared epsilon-insensitive loss.\n",
            "     |  \n",
            "     |  epsilon : float, optional (default=0.1)\n",
            "     |      Epsilon parameter in the epsilon-insensitive loss function. Note\n",
            "     |      that the value of this parameter depends on the scale of the target\n",
            "     |      variable y. If unsure, set ``epsilon=0``.\n",
            "     |  \n",
            "     |  dual : bool, (default=True)\n",
            "     |      Select the algorithm to either solve the dual or primal\n",
            "     |      optimization problem. Prefer dual=False when n_samples > n_features.\n",
            "     |  \n",
            "     |  tol : float, optional (default=1e-4)\n",
            "     |      Tolerance for stopping criteria.\n",
            "     |  \n",
            "     |  fit_intercept : boolean, optional (default=True)\n",
            "     |      Whether to calculate the intercept for this model. If set\n",
            "     |      to false, no intercept will be used in calculations\n",
            "     |      (i.e. data is expected to be already centered).\n",
            "     |  \n",
            "     |  intercept_scaling : float, optional (default=1)\n",
            "     |      When self.fit_intercept is True, instance vector x becomes\n",
            "     |      [x, self.intercept_scaling],\n",
            "     |      i.e. a \"synthetic\" feature with constant value equals to\n",
            "     |      intercept_scaling is appended to the instance vector.\n",
            "     |      The intercept becomes intercept_scaling * synthetic feature weight\n",
            "     |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
            "     |      as all other features.\n",
            "     |      To lessen the effect of regularization on synthetic feature weight\n",
            "     |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
            "     |  \n",
            "     |  verbose : int, (default=0)\n",
            "     |      Enable verbose output. Note that this setting takes advantage of a\n",
            "     |      per-process runtime setting in liblinear that, if enabled, may not work\n",
            "     |      properly in a multithreaded context.\n",
            "     |  \n",
            "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
            "     |      The seed of the pseudo random number generator to use when shuffling\n",
            "     |      the data.  If int, random_state is the seed used by the random number\n",
            "     |      generator; If RandomState instance, random_state is the random number\n",
            "     |      generator; If None, the random number generator is the RandomState\n",
            "     |      instance used by `np.random`.\n",
            "     |  \n",
            "     |  max_iter : int, (default=1000)\n",
            "     |      The maximum number of iterations to be run.\n",
            "     |  \n",
            "     |  Attributes\n",
            "     |  ----------\n",
            "     |  coef_ : array, shape = [n_features] if n_classes == 2 else [n_classes, n_features]\n",
            "     |      Weights assigned to the features (coefficients in the primal\n",
            "     |      problem). This is only available in the case of a linear kernel.\n",
            "     |  \n",
            "     |      `coef_` is a readonly property derived from `raw_coef_` that\n",
            "     |      follows the internal memory layout of liblinear.\n",
            "     |  \n",
            "     |  intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]\n",
            "     |      Constants in decision function.\n",
            "     |  \n",
            "     |  Examples\n",
            "     |  --------\n",
            "     |  >>> from sklearn.svm import LinearSVR\n",
            "     |  >>> from sklearn.datasets import make_regression\n",
            "     |  >>> X, y = make_regression(n_features=4, random_state=0)\n",
            "     |  >>> regr = LinearSVR(random_state=0)\n",
            "     |  >>> regr.fit(X, y)\n",
            "     |  LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
            "     |       intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n",
            "     |       random_state=0, tol=0.0001, verbose=0)\n",
            "     |  >>> print(regr.coef_)\n",
            "     |  [ 16.35750999  26.91499923  42.30652207  60.47843124]\n",
            "     |  >>> print(regr.intercept_)\n",
            "     |  [-4.29756543]\n",
            "     |  >>> print(regr.predict([[0, 0, 0, 0]]))\n",
            "     |  [-4.29756543]\n",
            "     |  \n",
            "     |  See also\n",
            "     |  --------\n",
            "     |  LinearSVC\n",
            "     |      Implementation of Support Vector Machine classifier using the\n",
            "     |      same library as this class (liblinear).\n",
            "     |  \n",
            "     |  SVR\n",
            "     |      Implementation of Support Vector Machine regression using libsvm:\n",
            "     |      the kernel can be non-linear but its SMO algorithm does not\n",
            "     |      scale to large number of samples as LinearSVC does.\n",
            "     |  \n",
            "     |  sklearn.linear_model.SGDRegressor\n",
            "     |      SGDRegressor can optimize the same cost function as LinearSVR\n",
            "     |      by adjusting the penalty and loss parameters. In addition it requires\n",
            "     |      less memory, allows incremental (online) learning, and implements\n",
            "     |      various loss functions and regularization regimes.\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      LinearSVR\n",
            "     |      sklearn.linear_model.base.LinearModel\n",
            "     |      abc.NewBase\n",
            "     |      sklearn.base.BaseEstimator\n",
            "     |      sklearn.base.RegressorMixin\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __init__(self, epsilon=0.0, tol=0.0001, C=1.0, loss='epsilon_insensitive', fit_intercept=True, intercept_scaling=1.0, dual=True, verbose=0, random_state=None, max_iter=1000)\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |  \n",
            "     |  fit(self, X, y, sample_weight=None)\n",
            "     |      Fit the model according to the given training data.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
            "     |          Training vector, where n_samples in the number of samples and\n",
            "     |          n_features is the number of features.\n",
            "     |      \n",
            "     |      y : array-like, shape = [n_samples]\n",
            "     |          Target vector relative to X\n",
            "     |      \n",
            "     |      sample_weight : array-like, shape = [n_samples], optional\n",
            "     |          Array of weights that are assigned to individual\n",
            "     |          samples. If not provided,\n",
            "     |          then each sample is given unit weight.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self : object\n",
            "     |          Returns self.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes defined here:\n",
            "     |  \n",
            "     |  __abstractmethods__ = frozenset()\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
            "     |  \n",
            "     |  predict(self, X)\n",
            "     |      Predict using the linear model\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
            "     |          Samples.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      C : array, shape = (n_samples,)\n",
            "     |          Returns predicted values.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
            "     |  \n",
            "     |  __getstate__(self)\n",
            "     |  \n",
            "     |  __repr__(self)\n",
            "     |      Return repr(self).\n",
            "     |  \n",
            "     |  __setstate__(self, state)\n",
            "     |  \n",
            "     |  get_params(self, deep=True)\n",
            "     |      Get parameters for this estimator.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      deep : boolean, optional\n",
            "     |          If True, will return the parameters for this estimator and\n",
            "     |          contained subobjects that are estimators.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      params : mapping of string to any\n",
            "     |          Parameter names mapped to their values.\n",
            "     |  \n",
            "     |  set_params(self, **params)\n",
            "     |      Set the parameters of this estimator.\n",
            "     |      \n",
            "     |      The method works on simple estimators as well as on nested objects\n",
            "     |      (such as pipelines). The latter have parameters of the form\n",
            "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
            "     |      component of a nested object.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
            "     |  \n",
            "     |  score(self, X, y, sample_weight=None)\n",
            "     |      Returns the coefficient of determination R^2 of the prediction.\n",
            "     |      \n",
            "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
            "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
            "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
            "     |      The best possible score is 1.0 and it can be negative (because the\n",
            "     |      model can be arbitrarily worse). A constant model that always\n",
            "     |      predicts the expected value of y, disregarding the input features,\n",
            "     |      would get a R^2 score of 0.0.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X : array-like, shape = (n_samples, n_features)\n",
            "     |          Test samples.\n",
            "     |      \n",
            "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
            "     |          True values for X.\n",
            "     |      \n",
            "     |      sample_weight : array-like, shape = [n_samples], optional\n",
            "     |          Sample weights.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      score : float\n",
            "     |          R^2 of self.predict(X) wrt. y.\n",
            "    \n",
            "    class NuSVC(sklearn.svm.base.BaseSVC)\n",
            "     |  Nu-Support Vector Classification.\n",
            "     |  \n",
            "     |  Similar to SVC but uses a parameter to control the number of support\n",
            "     |  vectors.\n",
            "     |  \n",
            "     |  The implementation is based on libsvm.\n",
            "     |  \n",
            "     |  Read more in the :ref:`User Guide <svm_classification>`.\n",
            "     |  \n",
            "     |  Parameters\n",
            "     |  ----------\n",
            "     |  nu : float, optional (default=0.5)\n",
            "     |      An upper bound on the fraction of training errors and a lower\n",
            "     |      bound of the fraction of support vectors. Should be in the\n",
            "     |      interval (0, 1].\n",
            "     |  \n",
            "     |  kernel : string, optional (default='rbf')\n",
            "     |       Specifies the kernel type to be used in the algorithm.\n",
            "     |       It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n",
            "     |       a callable.\n",
            "     |       If none is given, 'rbf' will be used. If a callable is given it is\n",
            "     |       used to precompute the kernel matrix.\n",
            "     |  \n",
            "     |  degree : int, optional (default=3)\n",
            "     |      Degree of the polynomial kernel function ('poly').\n",
            "     |      Ignored by all other kernels.\n",
            "     |  \n",
            "     |  gamma : float, optional (default='auto')\n",
            "     |      Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n",
            "     |      If gamma is 'auto' then 1/n_features will be used instead.\n",
            "     |  \n",
            "     |  coef0 : float, optional (default=0.0)\n",
            "     |      Independent term in kernel function.\n",
            "     |      It is only significant in 'poly' and 'sigmoid'.\n",
            "     |  \n",
            "     |  probability : boolean, optional (default=False)\n",
            "     |      Whether to enable probability estimates. This must be enabled prior\n",
            "     |      to calling `fit`, and will slow down that method.\n",
            "     |  \n",
            "     |  shrinking : boolean, optional (default=True)\n",
            "     |      Whether to use the shrinking heuristic.\n",
            "     |  \n",
            "     |  tol : float, optional (default=1e-3)\n",
            "     |      Tolerance for stopping criterion.\n",
            "     |  \n",
            "     |  cache_size : float, optional\n",
            "     |      Specify the size of the kernel cache (in MB).\n",
            "     |  \n",
            "     |  class_weight : {dict, 'balanced'}, optional\n",
            "     |      Set the parameter C of class i to class_weight[i]*C for\n",
            "     |      SVC. If not given, all classes are supposed to have\n",
            "     |      weight one. The \"balanced\" mode uses the values of y to automatically\n",
            "     |      adjust weights inversely proportional to class frequencies as\n",
            "     |      ``n_samples / (n_classes * np.bincount(y))``\n",
            "     |  \n",
            "     |  verbose : bool, default: False\n",
            "     |      Enable verbose output. Note that this setting takes advantage of a\n",
            "     |      per-process runtime setting in libsvm that, if enabled, may not work\n",
            "     |      properly in a multithreaded context.\n",
            "     |  \n",
            "     |  max_iter : int, optional (default=-1)\n",
            "     |      Hard limit on iterations within solver, or -1 for no limit.\n",
            "     |  \n",
            "     |  decision_function_shape : 'ovo', 'ovr', default='ovr'\n",
            "     |      Whether to return a one-vs-rest ('ovr') decision function of shape\n",
            "     |      (n_samples, n_classes) as all other classifiers, or the original\n",
            "     |      one-vs-one ('ovo') decision function of libsvm which has shape\n",
            "     |      (n_samples, n_classes * (n_classes - 1) / 2).\n",
            "     |  \n",
            "     |      .. versionchanged:: 0.19\n",
            "     |          decision_function_shape is 'ovr' by default.\n",
            "     |  \n",
            "     |      .. versionadded:: 0.17\n",
            "     |         *decision_function_shape='ovr'* is recommended.\n",
            "     |  \n",
            "     |      .. versionchanged:: 0.17\n",
            "     |         Deprecated *decision_function_shape='ovo' and None*.\n",
            "     |  \n",
            "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
            "     |      The seed of the pseudo random number generator to use when shuffling\n",
            "     |      the data.  If int, random_state is the seed used by the random number\n",
            "     |      generator; If RandomState instance, random_state is the random number\n",
            "     |      generator; If None, the random number generator is the RandomState\n",
            "     |      instance used by `np.random`.\n",
            "     |  \n",
            "     |  Attributes\n",
            "     |  ----------\n",
            "     |  support_ : array-like, shape = [n_SV]\n",
            "     |      Indices of support vectors.\n",
            "     |  \n",
            "     |  support_vectors_ : array-like, shape = [n_SV, n_features]\n",
            "     |      Support vectors.\n",
            "     |  \n",
            "     |  n_support_ : array-like, dtype=int32, shape = [n_class]\n",
            "     |      Number of support vectors for each class.\n",
            "     |  \n",
            "     |  dual_coef_ : array, shape = [n_class-1, n_SV]\n",
            "     |      Coefficients of the support vector in the decision function.\n",
            "     |      For multiclass, coefficient for all 1-vs-1 classifiers.\n",
            "     |      The layout of the coefficients in the multiclass case is somewhat\n",
            "     |      non-trivial. See the section about multi-class classification in\n",
            "     |      the SVM section of the User Guide for details.\n",
            "     |  \n",
            "     |  coef_ : array, shape = [n_class-1, n_features]\n",
            "     |      Weights assigned to the features (coefficients in the primal\n",
            "     |      problem). This is only available in the case of a linear kernel.\n",
            "     |  \n",
            "     |      `coef_` is readonly property derived from `dual_coef_` and\n",
            "     |      `support_vectors_`.\n",
            "     |  \n",
            "     |  intercept_ : array, shape = [n_class * (n_class-1) / 2]\n",
            "     |      Constants in decision function.\n",
            "     |  \n",
            "     |  Examples\n",
            "     |  --------\n",
            "     |  >>> import numpy as np\n",
            "     |  >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
            "     |  >>> y = np.array([1, 1, 2, 2])\n",
            "     |  >>> from sklearn.svm import NuSVC\n",
            "     |  >>> clf = NuSVC()\n",
            "     |  >>> clf.fit(X, y) #doctest: +NORMALIZE_WHITESPACE\n",
            "     |  NuSVC(cache_size=200, class_weight=None, coef0=0.0,\n",
            "     |        decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
            "     |        max_iter=-1, nu=0.5, probability=False, random_state=None,\n",
            "     |        shrinking=True, tol=0.001, verbose=False)\n",
            "     |  >>> print(clf.predict([[-0.8, -1]]))\n",
            "     |  [1]\n",
            "     |  \n",
            "     |  See also\n",
            "     |  --------\n",
            "     |  SVC\n",
            "     |      Support Vector Machine for classification using libsvm.\n",
            "     |  \n",
            "     |  LinearSVC\n",
            "     |      Scalable linear Support Vector Machine for classification using\n",
            "     |      liblinear.\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      NuSVC\n",
            "     |      sklearn.svm.base.BaseSVC\n",
            "     |      abc.NewBase\n",
            "     |      sklearn.svm.base.BaseLibSVM\n",
            "     |      abc.NewBase\n",
            "     |      sklearn.base.BaseEstimator\n",
            "     |      sklearn.base.ClassifierMixin\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __init__(self, nu=0.5, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', random_state=None)\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes defined here:\n",
            "     |  \n",
            "     |  __abstractmethods__ = frozenset()\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.svm.base.BaseSVC:\n",
            "     |  \n",
            "     |  decision_function(self, X)\n",
            "     |      Distance of the samples X to the separating hyperplane.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X : array-like, shape (n_samples, n_features)\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      X : array-like, shape (n_samples, n_classes * (n_classes-1) / 2)\n",
            "     |          Returns the decision function of the sample for each class\n",
            "     |          in the model.\n",
            "     |          If decision_function_shape='ovr', the shape is (n_samples,\n",
            "     |          n_classes)\n",
            "     |  \n",
            "     |  predict(self, X)\n",
            "     |      Perform classification on samples in X.\n",
            "     |      \n",
            "     |      For an one-class model, +1 or -1 is returned.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
            "     |          For kernel=\"precomputed\", the expected shape of X is\n",
            "     |          [n_samples_test, n_samples_train]\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      y_pred : array, shape (n_samples,)\n",
            "     |          Class labels for samples in X.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from sklearn.svm.base.BaseSVC:\n",
            "     |  \n",
            "     |  predict_log_proba\n",
            "     |      Compute log probabilities of possible outcomes for samples in X.\n",
            "     |      \n",
            "     |      The model need to have probability information computed at training\n",
            "     |      time: fit with attribute `probability` set to True.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X : array-like, shape (n_samples, n_features)\n",
            "     |          For kernel=\"precomputed\", the expected shape of X is\n",
            "     |          [n_samples_test, n_samples_train]\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      T : array-like, shape (n_samples, n_classes)\n",
            "     |          Returns the log-probabilities of the sample for each class in\n",
            "     |          the model. The columns correspond to the classes in sorted\n",
            "     |          order, as they appear in the attribute `classes_`.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      The probability model is created using cross validation, so\n",
            "     |      the results can be slightly different than those obtained by\n",
            "     |      predict. Also, it will produce meaningless results on very small\n",
            "     |      datasets.\n",
            "     |  \n",
            "     |  predict_proba\n",
            "     |      Compute probabilities of possible outcomes for samples in X.\n",
            "     |      \n",
            "     |      The model need to have probability information computed at training\n",
            "     |      time: fit with attribute `probability` set to True.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X : array-like, shape (n_samples, n_features)\n",
            "     |          For kernel=\"precomputed\", the expected shape of X is\n",
            "     |          [n_samples_test, n_samples_train]\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      T : array-like, shape (n_samples, n_classes)\n",
            "     |          Returns the probability of the sample for each class in\n",
            "     |          the model. The columns correspond to the classes in sorted\n",
            "     |          order, as they appear in the attribute `classes_`.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      The probability model is created using cross validation, so\n",
            "     |      the results can be slightly different than those obtained by\n",
            "     |      predict. Also, it will produce meaningless results on very small\n",
            "     |      datasets.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.svm.base.BaseLibSVM:\n",
            "     |  \n",
            "     |  fit(self, X, y, sample_weight=None)\n",
            "     |      Fit the SVM model according to the given training data.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
            "     |          Training vectors, where n_samples is the number of samples\n",
            "     |          and n_features is the number of features.\n",
            "     |          For kernel=\"precomputed\", the expected shape of X is\n",
            "     |          (n_samples, n_samples).\n",
            "     |      \n",
            "     |      y : array-like, shape (n_samples,)\n",
            "     |          Target values (class labels in classification, real numbers in\n",
            "     |          regression)\n",
            "     |      \n",
            "     |      sample_weight : array-like, shape (n_samples,)\n",
            "     |          Per-sample weights. Rescale C per sample. Higher weights\n",
            "     |          force the classifier to put more emphasis on these points.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self : object\n",
            "     |          Returns self.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      ------\n",
            "     |      If X and y are not C-ordered and contiguous arrays of np.float64 and\n",
            "     |      X is not a scipy.sparse.csr_matrix, X and/or y may be copied.\n",
            "     |      \n",
            "     |      If X is a dense array, then the other methods will not support sparse\n",
            "     |      matrices as input.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from sklearn.svm.base.BaseLibSVM:\n",
            "     |  \n",
            "     |  coef_\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
            "     |  \n",
            "     |  __getstate__(self)\n",
            "     |  \n",
            "     |  __repr__(self)\n",
            "     |      Return repr(self).\n",
            "     |  \n",
            "     |  __setstate__(self, state)\n",
            "     |  \n",
            "     |  get_params(self, deep=True)\n",
            "     |      Get parameters for this estimator.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      deep : boolean, optional\n",
            "     |          If True, will return the parameters for this estimator and\n",
            "     |          contained subobjects that are estimators.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      params : mapping of string to any\n",
            "     |          Parameter names mapped to their values.\n",
            "     |  \n",
            "     |  set_params(self, **params)\n",
            "     |      Set the parameters of this estimator.\n",
            "     |      \n",
            "     |      The method works on simple estimators as well as on nested objects\n",
            "     |      (such as pipelines). The latter have parameters of the form\n",
            "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
            "     |      component of a nested object.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
            "     |  \n",
            "     |  score(self, X, y, sample_weight=None)\n",
            "     |      Returns the mean accuracy on the given test data and labels.\n",
            "     |      \n",
            "     |      In multi-label classification, this is the subset accuracy\n",
            "     |      which is a harsh metric since you require for each sample that\n",
            "     |      each label set be correctly predicted.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X : array-like, shape = (n_samples, n_features)\n",
            "     |          Test samples.\n",
            "     |      \n",
            "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
            "     |          True labels for X.\n",
            "     |      \n",
            "     |      sample_weight : array-like, shape = [n_samples], optional\n",
            "     |          Sample weights.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      score : float\n",
            "     |          Mean accuracy of self.predict(X) wrt. y.\n",
            "    \n",
            "    class NuSVR(sklearn.svm.base.BaseLibSVM, sklearn.base.RegressorMixin)\n",
            "     |  Nu Support Vector Regression.\n",
            "     |  \n",
            "     |  Similar to NuSVC, for regression, uses a parameter nu to control\n",
            "     |  the number of support vectors. However, unlike NuSVC, where nu\n",
            "     |  replaces C, here nu replaces the parameter epsilon of epsilon-SVR.\n",
            "     |  \n",
            "     |  The implementation is based on libsvm.\n",
            "     |  \n",
            "     |  Read more in the :ref:`User Guide <svm_regression>`.\n",
            "     |  \n",
            "     |  Parameters\n",
            "     |  ----------\n",
            "     |  C : float, optional (default=1.0)\n",
            "     |      Penalty parameter C of the error term.\n",
            "     |  \n",
            "     |  nu : float, optional\n",
            "     |      An upper bound on the fraction of training errors and a lower bound of\n",
            "     |      the fraction of support vectors. Should be in the interval (0, 1].  By\n",
            "     |      default 0.5 will be taken.\n",
            "     |  \n",
            "     |  kernel : string, optional (default='rbf')\n",
            "     |       Specifies the kernel type to be used in the algorithm.\n",
            "     |       It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n",
            "     |       a callable.\n",
            "     |       If none is given, 'rbf' will be used. If a callable is given it is\n",
            "     |       used to precompute the kernel matrix.\n",
            "     |  \n",
            "     |  degree : int, optional (default=3)\n",
            "     |      Degree of the polynomial kernel function ('poly').\n",
            "     |      Ignored by all other kernels.\n",
            "     |  \n",
            "     |  gamma : float, optional (default='auto')\n",
            "     |      Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n",
            "     |      If gamma is 'auto' then 1/n_features will be used instead.\n",
            "     |  \n",
            "     |  coef0 : float, optional (default=0.0)\n",
            "     |      Independent term in kernel function.\n",
            "     |      It is only significant in 'poly' and 'sigmoid'.\n",
            "     |  \n",
            "     |  shrinking : boolean, optional (default=True)\n",
            "     |      Whether to use the shrinking heuristic.\n",
            "     |  \n",
            "     |  tol : float, optional (default=1e-3)\n",
            "     |      Tolerance for stopping criterion.\n",
            "     |  \n",
            "     |  cache_size : float, optional\n",
            "     |      Specify the size of the kernel cache (in MB).\n",
            "     |  \n",
            "     |  verbose : bool, default: False\n",
            "     |      Enable verbose output. Note that this setting takes advantage of a\n",
            "     |      per-process runtime setting in libsvm that, if enabled, may not work\n",
            "     |      properly in a multithreaded context.\n",
            "     |  \n",
            "     |  max_iter : int, optional (default=-1)\n",
            "     |      Hard limit on iterations within solver, or -1 for no limit.\n",
            "     |  \n",
            "     |  Attributes\n",
            "     |  ----------\n",
            "     |  support_ : array-like, shape = [n_SV]\n",
            "     |      Indices of support vectors.\n",
            "     |  \n",
            "     |  support_vectors_ : array-like, shape = [nSV, n_features]\n",
            "     |      Support vectors.\n",
            "     |  \n",
            "     |  dual_coef_ : array, shape = [1, n_SV]\n",
            "     |      Coefficients of the support vector in the decision function.\n",
            "     |  \n",
            "     |  coef_ : array, shape = [1, n_features]\n",
            "     |      Weights assigned to the features (coefficients in the primal\n",
            "     |      problem). This is only available in the case of a linear kernel.\n",
            "     |  \n",
            "     |      `coef_` is readonly property derived from `dual_coef_` and\n",
            "     |      `support_vectors_`.\n",
            "     |  \n",
            "     |  intercept_ : array, shape = [1]\n",
            "     |      Constants in decision function.\n",
            "     |  \n",
            "     |  Examples\n",
            "     |  --------\n",
            "     |  >>> from sklearn.svm import NuSVR\n",
            "     |  >>> import numpy as np\n",
            "     |  >>> n_samples, n_features = 10, 5\n",
            "     |  >>> np.random.seed(0)\n",
            "     |  >>> y = np.random.randn(n_samples)\n",
            "     |  >>> X = np.random.randn(n_samples, n_features)\n",
            "     |  >>> clf = NuSVR(C=1.0, nu=0.1)\n",
            "     |  >>> clf.fit(X, y)  #doctest: +NORMALIZE_WHITESPACE\n",
            "     |  NuSVR(C=1.0, cache_size=200, coef0=0.0, degree=3, gamma='auto',\n",
            "     |        kernel='rbf', max_iter=-1, nu=0.1, shrinking=True, tol=0.001,\n",
            "     |        verbose=False)\n",
            "     |  \n",
            "     |  See also\n",
            "     |  --------\n",
            "     |  NuSVC\n",
            "     |      Support Vector Machine for classification implemented with libsvm\n",
            "     |      with a parameter to control the number of support vectors.\n",
            "     |  \n",
            "     |  SVR\n",
            "     |      epsilon Support Vector Machine for regression implemented with libsvm.\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      NuSVR\n",
            "     |      sklearn.svm.base.BaseLibSVM\n",
            "     |      abc.NewBase\n",
            "     |      sklearn.base.BaseEstimator\n",
            "     |      sklearn.base.RegressorMixin\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __init__(self, nu=0.5, C=1.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, tol=0.001, cache_size=200, verbose=False, max_iter=-1)\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes defined here:\n",
            "     |  \n",
            "     |  __abstractmethods__ = frozenset()\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.svm.base.BaseLibSVM:\n",
            "     |  \n",
            "     |  fit(self, X, y, sample_weight=None)\n",
            "     |      Fit the SVM model according to the given training data.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
            "     |          Training vectors, where n_samples is the number of samples\n",
            "     |          and n_features is the number of features.\n",
            "     |          For kernel=\"precomputed\", the expected shape of X is\n",
            "     |          (n_samples, n_samples).\n",
            "     |      \n",
            "     |      y : array-like, shape (n_samples,)\n",
            "     |          Target values (class labels in classification, real numbers in\n",
            "     |          regression)\n",
            "     |      \n",
            "     |      sample_weight : array-like, shape (n_samples,)\n",
            "     |          Per-sample weights. Rescale C per sample. Higher weights\n",
            "     |          force the classifier to put more emphasis on these points.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self : object\n",
            "     |          Returns self.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      ------\n",
            "     |      If X and y are not C-ordered and contiguous arrays of np.float64 and\n",
            "     |      X is not a scipy.sparse.csr_matrix, X and/or y may be copied.\n",
            "     |      \n",
            "     |      If X is a dense array, then the other methods will not support sparse\n",
            "     |      matrices as input.\n",
            "     |  \n",
            "     |  predict(self, X)\n",
            "     |      Perform regression on samples in X.\n",
            "     |      \n",
            "     |      For an one-class model, +1 (inlier) or -1 (outlier) is returned.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
            "     |          For kernel=\"precomputed\", the expected shape of X is\n",
            "     |          (n_samples_test, n_samples_train).\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      y_pred : array, shape (n_samples,)\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from sklearn.svm.base.BaseLibSVM:\n",
            "     |  \n",
            "     |  coef_\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
            "     |  \n",
            "     |  __getstate__(self)\n",
            "     |  \n",
            "     |  __repr__(self)\n",
            "     |      Return repr(self).\n",
            "     |  \n",
            "     |  __setstate__(self, state)\n",
            "     |  \n",
            "     |  get_params(self, deep=True)\n",
            "     |      Get parameters for this estimator.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      deep : boolean, optional\n",
            "     |          If True, will return the parameters for this estimator and\n",
            "     |          contained subobjects that are estimators.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      params : mapping of string to any\n",
            "     |          Parameter names mapped to their values.\n",
            "     |  \n",
            "     |  set_params(self, **params)\n",
            "     |      Set the parameters of this estimator.\n",
            "     |      \n",
            "     |      The method works on simple estimators as well as on nested objects\n",
            "     |      (such as pipelines). The latter have parameters of the form\n",
            "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
            "     |      component of a nested object.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
            "     |  \n",
            "     |  score(self, X, y, sample_weight=None)\n",
            "     |      Returns the coefficient of determination R^2 of the prediction.\n",
            "     |      \n",
            "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
            "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
            "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
            "     |      The best possible score is 1.0 and it can be negative (because the\n",
            "     |      model can be arbitrarily worse). A constant model that always\n",
            "     |      predicts the expected value of y, disregarding the input features,\n",
            "     |      would get a R^2 score of 0.0.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X : array-like, shape = (n_samples, n_features)\n",
            "     |          Test samples.\n",
            "     |      \n",
            "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
            "     |          True values for X.\n",
            "     |      \n",
            "     |      sample_weight : array-like, shape = [n_samples], optional\n",
            "     |          Sample weights.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      score : float\n",
            "     |          R^2 of self.predict(X) wrt. y.\n",
            "    \n",
            "    class OneClassSVM(sklearn.svm.base.BaseLibSVM)\n",
            "     |  Unsupervised Outlier Detection.\n",
            "     |  \n",
            "     |  Estimate the support of a high-dimensional distribution.\n",
            "     |  \n",
            "     |  The implementation is based on libsvm.\n",
            "     |  \n",
            "     |  Read more in the :ref:`User Guide <svm_outlier_detection>`.\n",
            "     |  \n",
            "     |  Parameters\n",
            "     |  ----------\n",
            "     |  kernel : string, optional (default='rbf')\n",
            "     |       Specifies the kernel type to be used in the algorithm.\n",
            "     |       It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n",
            "     |       a callable.\n",
            "     |       If none is given, 'rbf' will be used. If a callable is given it is\n",
            "     |       used to precompute the kernel matrix.\n",
            "     |  \n",
            "     |  nu : float, optional\n",
            "     |      An upper bound on the fraction of training\n",
            "     |      errors and a lower bound of the fraction of support\n",
            "     |      vectors. Should be in the interval (0, 1]. By default 0.5\n",
            "     |      will be taken.\n",
            "     |  \n",
            "     |  degree : int, optional (default=3)\n",
            "     |      Degree of the polynomial kernel function ('poly').\n",
            "     |      Ignored by all other kernels.\n",
            "     |  \n",
            "     |  gamma : float, optional (default='auto')\n",
            "     |      Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n",
            "     |      If gamma is 'auto' then 1/n_features will be used instead.\n",
            "     |  \n",
            "     |  coef0 : float, optional (default=0.0)\n",
            "     |      Independent term in kernel function.\n",
            "     |      It is only significant in 'poly' and 'sigmoid'.\n",
            "     |  \n",
            "     |  tol : float, optional\n",
            "     |      Tolerance for stopping criterion.\n",
            "     |  \n",
            "     |  shrinking : boolean, optional\n",
            "     |      Whether to use the shrinking heuristic.\n",
            "     |  \n",
            "     |  cache_size : float, optional\n",
            "     |      Specify the size of the kernel cache (in MB).\n",
            "     |  \n",
            "     |  verbose : bool, default: False\n",
            "     |      Enable verbose output. Note that this setting takes advantage of a\n",
            "     |      per-process runtime setting in libsvm that, if enabled, may not work\n",
            "     |      properly in a multithreaded context.\n",
            "     |  \n",
            "     |  max_iter : int, optional (default=-1)\n",
            "     |      Hard limit on iterations within solver, or -1 for no limit.\n",
            "     |  \n",
            "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
            "     |      The seed of the pseudo random number generator to use when shuffling\n",
            "     |      the data.  If int, random_state is the seed used by the random number\n",
            "     |      generator; If RandomState instance, random_state is the random number\n",
            "     |      generator; If None, the random number generator is the RandomState\n",
            "     |      instance used by `np.random`.\n",
            "     |  \n",
            "     |  Attributes\n",
            "     |  ----------\n",
            "     |  support_ : array-like, shape = [n_SV]\n",
            "     |      Indices of support vectors.\n",
            "     |  \n",
            "     |  support_vectors_ : array-like, shape = [nSV, n_features]\n",
            "     |      Support vectors.\n",
            "     |  \n",
            "     |  dual_coef_ : array, shape = [1, n_SV]\n",
            "     |      Coefficients of the support vectors in the decision function.\n",
            "     |  \n",
            "     |  coef_ : array, shape = [1, n_features]\n",
            "     |      Weights assigned to the features (coefficients in the primal\n",
            "     |      problem). This is only available in the case of a linear kernel.\n",
            "     |  \n",
            "     |      `coef_` is readonly property derived from `dual_coef_` and\n",
            "     |      `support_vectors_`\n",
            "     |  \n",
            "     |  intercept_ : array, shape = [1,]\n",
            "     |      Constant in the decision function.\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      OneClassSVM\n",
            "     |      sklearn.svm.base.BaseLibSVM\n",
            "     |      abc.NewBase\n",
            "     |      sklearn.base.BaseEstimator\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __init__(self, kernel='rbf', degree=3, gamma='auto', coef0=0.0, tol=0.001, nu=0.5, shrinking=True, cache_size=200, verbose=False, max_iter=-1, random_state=None)\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |  \n",
            "     |  decision_function(self, X)\n",
            "     |      Signed distance to the separating hyperplane.\n",
            "     |      \n",
            "     |      Signed distance is positive for an inlier and negative for an outlier.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X : array-like, shape (n_samples, n_features)\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      X : array-like, shape (n_samples,)\n",
            "     |          Returns the decision function of the samples.\n",
            "     |  \n",
            "     |  fit(self, X, y=None, sample_weight=None, **params)\n",
            "     |      Detects the soft boundary of the set of samples X.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
            "     |          Set of samples, where n_samples is the number of samples and\n",
            "     |          n_features is the number of features.\n",
            "     |      \n",
            "     |      sample_weight : array-like, shape (n_samples,)\n",
            "     |          Per-sample weights. Rescale C per sample. Higher weights\n",
            "     |          force the classifier to put more emphasis on these points.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self : object\n",
            "     |          Returns self.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      If X is not a C-ordered contiguous array it is copied.\n",
            "     |  \n",
            "     |  predict(self, X)\n",
            "     |      Perform classification on samples in X.\n",
            "     |      \n",
            "     |      For an one-class model, +1 or -1 is returned.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
            "     |          For kernel=\"precomputed\", the expected shape of X is\n",
            "     |          [n_samples_test, n_samples_train]\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      y_pred : array, shape (n_samples,)\n",
            "     |          Class labels for samples in X.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes defined here:\n",
            "     |  \n",
            "     |  __abstractmethods__ = frozenset()\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from sklearn.svm.base.BaseLibSVM:\n",
            "     |  \n",
            "     |  coef_\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
            "     |  \n",
            "     |  __getstate__(self)\n",
            "     |  \n",
            "     |  __repr__(self)\n",
            "     |      Return repr(self).\n",
            "     |  \n",
            "     |  __setstate__(self, state)\n",
            "     |  \n",
            "     |  get_params(self, deep=True)\n",
            "     |      Get parameters for this estimator.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      deep : boolean, optional\n",
            "     |          If True, will return the parameters for this estimator and\n",
            "     |          contained subobjects that are estimators.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      params : mapping of string to any\n",
            "     |          Parameter names mapped to their values.\n",
            "     |  \n",
            "     |  set_params(self, **params)\n",
            "     |      Set the parameters of this estimator.\n",
            "     |      \n",
            "     |      The method works on simple estimators as well as on nested objects\n",
            "     |      (such as pipelines). The latter have parameters of the form\n",
            "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
            "     |      component of a nested object.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "    \n",
            "    class SVC(sklearn.svm.base.BaseSVC)\n",
            "     |  C-Support Vector Classification.\n",
            "     |  \n",
            "     |  The implementation is based on libsvm. The fit time complexity\n",
            "     |  is more than quadratic with the number of samples which makes it hard\n",
            "     |  to scale to dataset with more than a couple of 10000 samples.\n",
            "     |  \n",
            "     |  The multiclass support is handled according to a one-vs-one scheme.\n",
            "     |  \n",
            "     |  For details on the precise mathematical formulation of the provided\n",
            "     |  kernel functions and how `gamma`, `coef0` and `degree` affect each\n",
            "     |  other, see the corresponding section in the narrative documentation:\n",
            "     |  :ref:`svm_kernels`.\n",
            "     |  \n",
            "     |  Read more in the :ref:`User Guide <svm_classification>`.\n",
            "     |  \n",
            "     |  Parameters\n",
            "     |  ----------\n",
            "     |  C : float, optional (default=1.0)\n",
            "     |      Penalty parameter C of the error term.\n",
            "     |  \n",
            "     |  kernel : string, optional (default='rbf')\n",
            "     |       Specifies the kernel type to be used in the algorithm.\n",
            "     |       It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n",
            "     |       a callable.\n",
            "     |       If none is given, 'rbf' will be used. If a callable is given it is\n",
            "     |       used to pre-compute the kernel matrix from data matrices; that matrix\n",
            "     |       should be an array of shape ``(n_samples, n_samples)``.\n",
            "     |  \n",
            "     |  degree : int, optional (default=3)\n",
            "     |      Degree of the polynomial kernel function ('poly').\n",
            "     |      Ignored by all other kernels.\n",
            "     |  \n",
            "     |  gamma : float, optional (default='auto')\n",
            "     |      Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n",
            "     |      If gamma is 'auto' then 1/n_features will be used instead.\n",
            "     |  \n",
            "     |  coef0 : float, optional (default=0.0)\n",
            "     |      Independent term in kernel function.\n",
            "     |      It is only significant in 'poly' and 'sigmoid'.\n",
            "     |  \n",
            "     |  probability : boolean, optional (default=False)\n",
            "     |      Whether to enable probability estimates. This must be enabled prior\n",
            "     |      to calling `fit`, and will slow down that method.\n",
            "     |  \n",
            "     |  shrinking : boolean, optional (default=True)\n",
            "     |      Whether to use the shrinking heuristic.\n",
            "     |  \n",
            "     |  tol : float, optional (default=1e-3)\n",
            "     |      Tolerance for stopping criterion.\n",
            "     |  \n",
            "     |  cache_size : float, optional\n",
            "     |      Specify the size of the kernel cache (in MB).\n",
            "     |  \n",
            "     |  class_weight : {dict, 'balanced'}, optional\n",
            "     |      Set the parameter C of class i to class_weight[i]*C for\n",
            "     |      SVC. If not given, all classes are supposed to have\n",
            "     |      weight one.\n",
            "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
            "     |      weights inversely proportional to class frequencies in the input data\n",
            "     |      as ``n_samples / (n_classes * np.bincount(y))``\n",
            "     |  \n",
            "     |  verbose : bool, default: False\n",
            "     |      Enable verbose output. Note that this setting takes advantage of a\n",
            "     |      per-process runtime setting in libsvm that, if enabled, may not work\n",
            "     |      properly in a multithreaded context.\n",
            "     |  \n",
            "     |  max_iter : int, optional (default=-1)\n",
            "     |      Hard limit on iterations within solver, or -1 for no limit.\n",
            "     |  \n",
            "     |  decision_function_shape : 'ovo', 'ovr', default='ovr'\n",
            "     |      Whether to return a one-vs-rest ('ovr') decision function of shape\n",
            "     |      (n_samples, n_classes) as all other classifiers, or the original\n",
            "     |      one-vs-one ('ovo') decision function of libsvm which has shape\n",
            "     |      (n_samples, n_classes * (n_classes - 1) / 2).\n",
            "     |  \n",
            "     |      .. versionchanged:: 0.19\n",
            "     |          decision_function_shape is 'ovr' by default.\n",
            "     |  \n",
            "     |      .. versionadded:: 0.17\n",
            "     |         *decision_function_shape='ovr'* is recommended.\n",
            "     |  \n",
            "     |      .. versionchanged:: 0.17\n",
            "     |         Deprecated *decision_function_shape='ovo' and None*.\n",
            "     |  \n",
            "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
            "     |      The seed of the pseudo random number generator to use when shuffling\n",
            "     |      the data.  If int, random_state is the seed used by the random number\n",
            "     |      generator; If RandomState instance, random_state is the random number\n",
            "     |      generator; If None, the random number generator is the RandomState\n",
            "     |      instance used by `np.random`.\n",
            "     |  \n",
            "     |  Attributes\n",
            "     |  ----------\n",
            "     |  support_ : array-like, shape = [n_SV]\n",
            "     |      Indices of support vectors.\n",
            "     |  \n",
            "     |  support_vectors_ : array-like, shape = [n_SV, n_features]\n",
            "     |      Support vectors.\n",
            "     |  \n",
            "     |  n_support_ : array-like, dtype=int32, shape = [n_class]\n",
            "     |      Number of support vectors for each class.\n",
            "     |  \n",
            "     |  dual_coef_ : array, shape = [n_class-1, n_SV]\n",
            "     |      Coefficients of the support vector in the decision function.\n",
            "     |      For multiclass, coefficient for all 1-vs-1 classifiers.\n",
            "     |      The layout of the coefficients in the multiclass case is somewhat\n",
            "     |      non-trivial. See the section about multi-class classification in the\n",
            "     |      SVM section of the User Guide for details.\n",
            "     |  \n",
            "     |  coef_ : array, shape = [n_class-1, n_features]\n",
            "     |      Weights assigned to the features (coefficients in the primal\n",
            "     |      problem). This is only available in the case of a linear kernel.\n",
            "     |  \n",
            "     |      `coef_` is a readonly property derived from `dual_coef_` and\n",
            "     |      `support_vectors_`.\n",
            "     |  \n",
            "     |  intercept_ : array, shape = [n_class * (n_class-1) / 2]\n",
            "     |      Constants in decision function.\n",
            "     |  \n",
            "     |  Examples\n",
            "     |  --------\n",
            "     |  >>> import numpy as np\n",
            "     |  >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
            "     |  >>> y = np.array([1, 1, 2, 2])\n",
            "     |  >>> from sklearn.svm import SVC\n",
            "     |  >>> clf = SVC()\n",
            "     |  >>> clf.fit(X, y) #doctest: +NORMALIZE_WHITESPACE\n",
            "     |  SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
            "     |      decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
            "     |      max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
            "     |      tol=0.001, verbose=False)\n",
            "     |  >>> print(clf.predict([[-0.8, -1]]))\n",
            "     |  [1]\n",
            "     |  \n",
            "     |  See also\n",
            "     |  --------\n",
            "     |  SVR\n",
            "     |      Support Vector Machine for Regression implemented using libsvm.\n",
            "     |  \n",
            "     |  LinearSVC\n",
            "     |      Scalable Linear Support Vector Machine for classification\n",
            "     |      implemented using liblinear. Check the See also section of\n",
            "     |      LinearSVC for more comparison element.\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      SVC\n",
            "     |      sklearn.svm.base.BaseSVC\n",
            "     |      abc.NewBase\n",
            "     |      sklearn.svm.base.BaseLibSVM\n",
            "     |      abc.NewBase\n",
            "     |      sklearn.base.BaseEstimator\n",
            "     |      sklearn.base.ClassifierMixin\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __init__(self, C=1.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', random_state=None)\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes defined here:\n",
            "     |  \n",
            "     |  __abstractmethods__ = frozenset()\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.svm.base.BaseSVC:\n",
            "     |  \n",
            "     |  decision_function(self, X)\n",
            "     |      Distance of the samples X to the separating hyperplane.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X : array-like, shape (n_samples, n_features)\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      X : array-like, shape (n_samples, n_classes * (n_classes-1) / 2)\n",
            "     |          Returns the decision function of the sample for each class\n",
            "     |          in the model.\n",
            "     |          If decision_function_shape='ovr', the shape is (n_samples,\n",
            "     |          n_classes)\n",
            "     |  \n",
            "     |  predict(self, X)\n",
            "     |      Perform classification on samples in X.\n",
            "     |      \n",
            "     |      For an one-class model, +1 or -1 is returned.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
            "     |          For kernel=\"precomputed\", the expected shape of X is\n",
            "     |          [n_samples_test, n_samples_train]\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      y_pred : array, shape (n_samples,)\n",
            "     |          Class labels for samples in X.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from sklearn.svm.base.BaseSVC:\n",
            "     |  \n",
            "     |  predict_log_proba\n",
            "     |      Compute log probabilities of possible outcomes for samples in X.\n",
            "     |      \n",
            "     |      The model need to have probability information computed at training\n",
            "     |      time: fit with attribute `probability` set to True.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X : array-like, shape (n_samples, n_features)\n",
            "     |          For kernel=\"precomputed\", the expected shape of X is\n",
            "     |          [n_samples_test, n_samples_train]\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      T : array-like, shape (n_samples, n_classes)\n",
            "     |          Returns the log-probabilities of the sample for each class in\n",
            "     |          the model. The columns correspond to the classes in sorted\n",
            "     |          order, as they appear in the attribute `classes_`.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      The probability model is created using cross validation, so\n",
            "     |      the results can be slightly different than those obtained by\n",
            "     |      predict. Also, it will produce meaningless results on very small\n",
            "     |      datasets.\n",
            "     |  \n",
            "     |  predict_proba\n",
            "     |      Compute probabilities of possible outcomes for samples in X.\n",
            "     |      \n",
            "     |      The model need to have probability information computed at training\n",
            "     |      time: fit with attribute `probability` set to True.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X : array-like, shape (n_samples, n_features)\n",
            "     |          For kernel=\"precomputed\", the expected shape of X is\n",
            "     |          [n_samples_test, n_samples_train]\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      T : array-like, shape (n_samples, n_classes)\n",
            "     |          Returns the probability of the sample for each class in\n",
            "     |          the model. The columns correspond to the classes in sorted\n",
            "     |          order, as they appear in the attribute `classes_`.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      The probability model is created using cross validation, so\n",
            "     |      the results can be slightly different than those obtained by\n",
            "     |      predict. Also, it will produce meaningless results on very small\n",
            "     |      datasets.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.svm.base.BaseLibSVM:\n",
            "     |  \n",
            "     |  fit(self, X, y, sample_weight=None)\n",
            "     |      Fit the SVM model according to the given training data.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
            "     |          Training vectors, where n_samples is the number of samples\n",
            "     |          and n_features is the number of features.\n",
            "     |          For kernel=\"precomputed\", the expected shape of X is\n",
            "     |          (n_samples, n_samples).\n",
            "     |      \n",
            "     |      y : array-like, shape (n_samples,)\n",
            "     |          Target values (class labels in classification, real numbers in\n",
            "     |          regression)\n",
            "     |      \n",
            "     |      sample_weight : array-like, shape (n_samples,)\n",
            "     |          Per-sample weights. Rescale C per sample. Higher weights\n",
            "     |          force the classifier to put more emphasis on these points.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self : object\n",
            "     |          Returns self.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      ------\n",
            "     |      If X and y are not C-ordered and contiguous arrays of np.float64 and\n",
            "     |      X is not a scipy.sparse.csr_matrix, X and/or y may be copied.\n",
            "     |      \n",
            "     |      If X is a dense array, then the other methods will not support sparse\n",
            "     |      matrices as input.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from sklearn.svm.base.BaseLibSVM:\n",
            "     |  \n",
            "     |  coef_\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
            "     |  \n",
            "     |  __getstate__(self)\n",
            "     |  \n",
            "     |  __repr__(self)\n",
            "     |      Return repr(self).\n",
            "     |  \n",
            "     |  __setstate__(self, state)\n",
            "     |  \n",
            "     |  get_params(self, deep=True)\n",
            "     |      Get parameters for this estimator.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      deep : boolean, optional\n",
            "     |          If True, will return the parameters for this estimator and\n",
            "     |          contained subobjects that are estimators.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      params : mapping of string to any\n",
            "     |          Parameter names mapped to their values.\n",
            "     |  \n",
            "     |  set_params(self, **params)\n",
            "     |      Set the parameters of this estimator.\n",
            "     |      \n",
            "     |      The method works on simple estimators as well as on nested objects\n",
            "     |      (such as pipelines). The latter have parameters of the form\n",
            "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
            "     |      component of a nested object.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
            "     |  \n",
            "     |  score(self, X, y, sample_weight=None)\n",
            "     |      Returns the mean accuracy on the given test data and labels.\n",
            "     |      \n",
            "     |      In multi-label classification, this is the subset accuracy\n",
            "     |      which is a harsh metric since you require for each sample that\n",
            "     |      each label set be correctly predicted.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X : array-like, shape = (n_samples, n_features)\n",
            "     |          Test samples.\n",
            "     |      \n",
            "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
            "     |          True labels for X.\n",
            "     |      \n",
            "     |      sample_weight : array-like, shape = [n_samples], optional\n",
            "     |          Sample weights.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      score : float\n",
            "     |          Mean accuracy of self.predict(X) wrt. y.\n",
            "    \n",
            "    class SVR(sklearn.svm.base.BaseLibSVM, sklearn.base.RegressorMixin)\n",
            "     |  Epsilon-Support Vector Regression.\n",
            "     |  \n",
            "     |  The free parameters in the model are C and epsilon.\n",
            "     |  \n",
            "     |  The implementation is based on libsvm.\n",
            "     |  \n",
            "     |  Read more in the :ref:`User Guide <svm_regression>`.\n",
            "     |  \n",
            "     |  Parameters\n",
            "     |  ----------\n",
            "     |  C : float, optional (default=1.0)\n",
            "     |      Penalty parameter C of the error term.\n",
            "     |  \n",
            "     |  epsilon : float, optional (default=0.1)\n",
            "     |       Epsilon in the epsilon-SVR model. It specifies the epsilon-tube\n",
            "     |       within which no penalty is associated in the training loss function\n",
            "     |       with points predicted within a distance epsilon from the actual\n",
            "     |       value.\n",
            "     |  \n",
            "     |  kernel : string, optional (default='rbf')\n",
            "     |       Specifies the kernel type to be used in the algorithm.\n",
            "     |       It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n",
            "     |       a callable.\n",
            "     |       If none is given, 'rbf' will be used. If a callable is given it is\n",
            "     |       used to precompute the kernel matrix.\n",
            "     |  \n",
            "     |  degree : int, optional (default=3)\n",
            "     |      Degree of the polynomial kernel function ('poly').\n",
            "     |      Ignored by all other kernels.\n",
            "     |  \n",
            "     |  gamma : float, optional (default='auto')\n",
            "     |      Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n",
            "     |      If gamma is 'auto' then 1/n_features will be used instead.\n",
            "     |  \n",
            "     |  coef0 : float, optional (default=0.0)\n",
            "     |      Independent term in kernel function.\n",
            "     |      It is only significant in 'poly' and 'sigmoid'.\n",
            "     |  \n",
            "     |  shrinking : boolean, optional (default=True)\n",
            "     |      Whether to use the shrinking heuristic.\n",
            "     |  \n",
            "     |  tol : float, optional (default=1e-3)\n",
            "     |      Tolerance for stopping criterion.\n",
            "     |  \n",
            "     |  cache_size : float, optional\n",
            "     |      Specify the size of the kernel cache (in MB).\n",
            "     |  \n",
            "     |  verbose : bool, default: False\n",
            "     |      Enable verbose output. Note that this setting takes advantage of a\n",
            "     |      per-process runtime setting in libsvm that, if enabled, may not work\n",
            "     |      properly in a multithreaded context.\n",
            "     |  \n",
            "     |  max_iter : int, optional (default=-1)\n",
            "     |      Hard limit on iterations within solver, or -1 for no limit.\n",
            "     |  \n",
            "     |  Attributes\n",
            "     |  ----------\n",
            "     |  support_ : array-like, shape = [n_SV]\n",
            "     |      Indices of support vectors.\n",
            "     |  \n",
            "     |  support_vectors_ : array-like, shape = [nSV, n_features]\n",
            "     |      Support vectors.\n",
            "     |  \n",
            "     |  dual_coef_ : array, shape = [1, n_SV]\n",
            "     |      Coefficients of the support vector in the decision function.\n",
            "     |  \n",
            "     |  coef_ : array, shape = [1, n_features]\n",
            "     |      Weights assigned to the features (coefficients in the primal\n",
            "     |      problem). This is only available in the case of a linear kernel.\n",
            "     |  \n",
            "     |      `coef_` is readonly property derived from `dual_coef_` and\n",
            "     |      `support_vectors_`.\n",
            "     |  \n",
            "     |  intercept_ : array, shape = [1]\n",
            "     |      Constants in decision function.\n",
            "     |  \n",
            "     |  sample_weight : array-like, shape = [n_samples]\n",
            "     |          Individual weights for each sample\n",
            "     |  \n",
            "     |  Examples\n",
            "     |  --------\n",
            "     |  >>> from sklearn.svm import SVR\n",
            "     |  >>> import numpy as np\n",
            "     |  >>> n_samples, n_features = 10, 5\n",
            "     |  >>> np.random.seed(0)\n",
            "     |  >>> y = np.random.randn(n_samples)\n",
            "     |  >>> X = np.random.randn(n_samples, n_features)\n",
            "     |  >>> clf = SVR(C=1.0, epsilon=0.2)\n",
            "     |  >>> clf.fit(X, y) #doctest: +NORMALIZE_WHITESPACE\n",
            "     |  SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.2, gamma='auto',\n",
            "     |      kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
            "     |  \n",
            "     |  See also\n",
            "     |  --------\n",
            "     |  NuSVR\n",
            "     |      Support Vector Machine for regression implemented using libsvm\n",
            "     |      using a parameter to control the number of support vectors.\n",
            "     |  \n",
            "     |  LinearSVR\n",
            "     |      Scalable Linear Support Vector Machine for regression\n",
            "     |      implemented using liblinear.\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      SVR\n",
            "     |      sklearn.svm.base.BaseLibSVM\n",
            "     |      abc.NewBase\n",
            "     |      sklearn.base.BaseEstimator\n",
            "     |      sklearn.base.RegressorMixin\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __init__(self, kernel='rbf', degree=3, gamma='auto', coef0=0.0, tol=0.001, C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1)\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes defined here:\n",
            "     |  \n",
            "     |  __abstractmethods__ = frozenset()\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.svm.base.BaseLibSVM:\n",
            "     |  \n",
            "     |  fit(self, X, y, sample_weight=None)\n",
            "     |      Fit the SVM model according to the given training data.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
            "     |          Training vectors, where n_samples is the number of samples\n",
            "     |          and n_features is the number of features.\n",
            "     |          For kernel=\"precomputed\", the expected shape of X is\n",
            "     |          (n_samples, n_samples).\n",
            "     |      \n",
            "     |      y : array-like, shape (n_samples,)\n",
            "     |          Target values (class labels in classification, real numbers in\n",
            "     |          regression)\n",
            "     |      \n",
            "     |      sample_weight : array-like, shape (n_samples,)\n",
            "     |          Per-sample weights. Rescale C per sample. Higher weights\n",
            "     |          force the classifier to put more emphasis on these points.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self : object\n",
            "     |          Returns self.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      ------\n",
            "     |      If X and y are not C-ordered and contiguous arrays of np.float64 and\n",
            "     |      X is not a scipy.sparse.csr_matrix, X and/or y may be copied.\n",
            "     |      \n",
            "     |      If X is a dense array, then the other methods will not support sparse\n",
            "     |      matrices as input.\n",
            "     |  \n",
            "     |  predict(self, X)\n",
            "     |      Perform regression on samples in X.\n",
            "     |      \n",
            "     |      For an one-class model, +1 (inlier) or -1 (outlier) is returned.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
            "     |          For kernel=\"precomputed\", the expected shape of X is\n",
            "     |          (n_samples_test, n_samples_train).\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      y_pred : array, shape (n_samples,)\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from sklearn.svm.base.BaseLibSVM:\n",
            "     |  \n",
            "     |  coef_\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
            "     |  \n",
            "     |  __getstate__(self)\n",
            "     |  \n",
            "     |  __repr__(self)\n",
            "     |      Return repr(self).\n",
            "     |  \n",
            "     |  __setstate__(self, state)\n",
            "     |  \n",
            "     |  get_params(self, deep=True)\n",
            "     |      Get parameters for this estimator.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      deep : boolean, optional\n",
            "     |          If True, will return the parameters for this estimator and\n",
            "     |          contained subobjects that are estimators.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      params : mapping of string to any\n",
            "     |          Parameter names mapped to their values.\n",
            "     |  \n",
            "     |  set_params(self, **params)\n",
            "     |      Set the parameters of this estimator.\n",
            "     |      \n",
            "     |      The method works on simple estimators as well as on nested objects\n",
            "     |      (such as pipelines). The latter have parameters of the form\n",
            "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
            "     |      component of a nested object.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
            "     |  \n",
            "     |  score(self, X, y, sample_weight=None)\n",
            "     |      Returns the coefficient of determination R^2 of the prediction.\n",
            "     |      \n",
            "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
            "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
            "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
            "     |      The best possible score is 1.0 and it can be negative (because the\n",
            "     |      model can be arbitrarily worse). A constant model that always\n",
            "     |      predicts the expected value of y, disregarding the input features,\n",
            "     |      would get a R^2 score of 0.0.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X : array-like, shape = (n_samples, n_features)\n",
            "     |          Test samples.\n",
            "     |      \n",
            "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
            "     |          True values for X.\n",
            "     |      \n",
            "     |      sample_weight : array-like, shape = [n_samples], optional\n",
            "     |          Sample weights.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      score : float\n",
            "     |          R^2 of self.predict(X) wrt. y.\n",
            "\n",
            "FILE\n",
            "    /usr/local/lib/python3.6/dist-packages/sklearn/svm/classes.py\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XOV5vUm7pETJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "#from parser import load_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PuSbsN25rnKq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e91eea86-6ee7-4ceb-fc1a-d42969d6da28"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "(traindata, trainlabels), (testdata, testlabels) = mnist.load_data()\n",
        "traindata = traindata.reshape(traindata.shape[0],traindata.shape[1],traindata.shape[2],1)\n",
        "trainlabels = keras.utils.to_categorical(trainlabels, 10)\n",
        "print(traindata.shape)\n",
        "print(trainlabels.shape)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28, 1)\n",
            "(60000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pJwM3xJfsctv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6b127c49-d117-478b-e625-de877965e306"
      },
      "cell_type": "code",
      "source": [
        "model = keras.Sequential()\n",
        "#model.add(keras.layers.Flatten())\n",
        "\n",
        "#model.add(layers.Convolution2D(32,3,3 input_shape=(img_width, img_height,3)))\n",
        "model.add(keras.layers.Conv2D(10,3,3, input_shape=(28, 28,1)))\n",
        "#keras.layers.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', \n",
        "#                    bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
        "\n",
        "model.add(keras.layers.Activation('relu'))\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "#model.add(keras.layers.Conv2D(10,3,3, input_shape=(28, 28,1)))\n",
        "#model.add(keras.layers.Activation('relu'))\n",
        "#model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "#model.add(keras.layers.Conv2D(10,3,3, input_shape=(28, 28,1)))\n",
        "#model.add(keras.layers.Activation('relu'))\n",
        "#model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "#model.add(keras.layers.Flatten())\n",
        "\n",
        "#model.add(keras.layers.Dense(64))\n",
        "#model.add(keras.layers.Activation('relu'))\n",
        "\n",
        "#model.add(keras.layers.Dropout(0.5))\n",
        "\n",
        "#keras.layers.Dropout(rate, noise_shape=None, seed=None)\n",
        "#consists in randomly setting a fraction rate of input units to 0 at each update during training time,\n",
        "\n",
        "model.add(keras.layers.Dense(10))\n",
        "model.add(keras.layers.Activation('softmax'))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), input_shape=(28, 28, 1...)`\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "a4f5jB-h8iqb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "             optimizer='rmsprop',\n",
        "             metrics=['accuracy'])\n",
        "#keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0) RMSProp optimizer. It is recommended to leave the parameters of this optimizer at their default values \n",
        "#                        (except the learning rate, which can be freely tuned). This optimizer is usually a good choice for recurrent neural networks.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hhgc8ojG-Ln3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit(traindata,trainlabels, epochs=2)\n",
        "#model.fit_generator(traindata.reshape(traindata,trainlabels, epochs=2 ))\n",
        "#model.save_weights('models/blah.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}